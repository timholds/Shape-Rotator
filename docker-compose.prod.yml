services:
  # nginx:
  #   image: nginx:alpine
  #   ports:
  #     - "8080:80"   # Changed from 80:80
  #     - "8443:443"  # Changed from 443:443
  #   volumes:
  #     - ./nginx.conf:/etc/nginx/nginx.conf:ro
  #     - /etc/ssl/shape-rotator.crt:/etc/ssl/shape-rotator.crt:ro
  #     - /etc/ssl/shape-rotator.key:/etc/ssl/shape-rotator.key:ro
  #   depends_on:
  #     - frontend
  #     - backend
  #   networks:
  #     - app-network
  #   restart: always

  frontend:
    image: ${DOCKER_USERNAME}/shape-frontend:${BUILD_ID:-latest}
    expose:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=https://${DOMAIN}/api
      - NODE_ENV=production
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app-network
    restart: always

  backend:
    image: ${DOCKER_USERNAME}/shape-backend:${BUILD_ID:-latest}
    expose:
      - "8000:8000"
    volumes:
      - ${PROJECT_DIR:-/root}/media:/app/media
      - ${PROJECT_DIR:-/root}/training_data:/app/training_data
    environment:
      - DOMAIN=${DOMAIN}
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - SYSTEM_PROMPT_PATH=system_prompt.txt
      - OLLAMA_HOST=http://ollama:11434
      - DO_BUCKET_ID=${DO_BUCKET_ID}
      - DO_BUCKET_SECRET=${DO_BUCKET_SECRET}
      - DO_BUCKET_NAME=${DO_BUCKET_NAME}
    networks:
      - app-network
    depends_on:
      ollama-init:
        condition: service_completed_successfully
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/docs"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: always

  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - app-network
    deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s
    restart: always

  ollama-init:
    image: curlimages/curl:latest
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      - app-network
    command: >
      command: >
        sh -c "
        echo 'Checking if mistral:latest-cuda model exists...' &&
        if ! curl -s http://ollama:11434/api/tags | grep -q '\"name\":\"mistral:latest-cuda\"'; then
          echo 'Pulling mistral CUDA model...' &&
          curl -X POST http://ollama:11434/api/pull -d '{\"name\":\"mistral:latest-cuda\"}' &&
          echo 'Model pull completed'
        else
          echo 'Mistral CUDA model already exists'
        fi"
    restart: "no"  # Only run once

networks:
  app-network:
    driver: bridge

volumes:
  ollama_data: